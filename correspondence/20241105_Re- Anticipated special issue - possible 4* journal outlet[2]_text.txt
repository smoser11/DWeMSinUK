We need to ensure that the ego-centric  survey questions used in the RDS are comparable to alter-centred questions used in NSUM.  Here are ways to aggregate (or combine) questions to create new comparable questions for both methods, from my co-author, CE:
""""

Date: 20241105
Subject: Re: Anticipated special issue - possible 4* journal outlet

Hi Scott,

Here is the list of questions (from most confident at the top to least confident at the bottom):

RDS = NSUM

Q70 5f2 = Q71 5f3
Q39 5b4 and Q42 5b7 = Q43 5b8
Q45 5c2 and Q47 5c4 and Q48 5c5 = Q49 5c6
Q61 5d8 and Q62 5d9 = Q64 5d11 (excessive hours is covered, but we didn't ask an RDS question about annual leave which is included in Q 64 5d11)
Q78 5f10 (answers coded (1) No) = Q79 5f11

The known network size question I identified is Q13 2f (related to the number of domestic workers for whom you have contact details in your phone)

Best wishes
Caroline
"""

 Use the existing CLAUDE.md file to get up to speed on this project, 
  please.
  
  
  
  
  Thank you.  I want combine, refine, and organize the R: codebase.  For 
  example there are currently R: scirpts in the R folder as well as in the 
  `analysis` and `data-processing` subfolders.  I think the latter two 
  contain important and useful code, but it is all very disorganized.
  
  In addition, additional data-work is needed to ensure that the ego-centric  survey questions used in the RDS are comparable to alter-centred questions used in NSUM.  Here are ways to aggregate (or combine) questions to create new comparable questions for both methods, from my co-author, CE:
""""

Date: 20241105
Subject: Re: Anticipated special issue - possible 4* journal outlet

Hi Scott,

Here is the list of questions (from most confident at the top to least confident at the bottom):

RDS = NSUM

Q70 5f2 = Q71 5f3
Q39 5b4 and Q42 5b7 = Q43 5b8
Q45 5c2 and Q47 5c4 and Q48 5c5 = Q49 5c6
Q61 5d8 and Q62 5d9 = Q64 5d11 (excessive hours is covered, but we didn't ask an RDS question about annual leave which is included in Q 64 5d11)
Q78 5f10 (answers coded (1) No) = Q79 5f11

The known network size question I identified is Q13 2f (related to the number of domestic workers for whom you have contact details in your phone)

Best wishes
Caroline
"""
  
  

1. Technical Analysis Steps

Your notes say:

Neighborhood bootstrap for RDS

Feehan & Salganik weighted NSUM

Match question sets (Q70/71, Q39+42/43, Q45+47+48/49, Q61+62/64, Q78+79)

‚úÖ This matches perfectly with the ‚Äúpreferred RDS + NSUM with weights‚Äù plan I outlined.
üîß Adjustment: Instead of just Q36/Q80, you‚Äôve identified additional question groupings for NSUM matching. That means in your Findings, you‚Äôll have a set of outcomes: pay issues, document withholding, abuse/threats, excessive hours, access to help ‚Äî plus your Q36/Q80 binary and risk index.

üëâ Suggestion:
Focus first on RDS + NSUM for one or two outcomes (say, pay below minimum wage and document withholding). Once that pipeline works, scale to the rest. Otherwise you risk tinkering forever on all at once.

2. Population Parameters

Your notes say:

980,000 as UK DW population (EU)

44,360 as NRM referrals baseline

Handle ‚Äúdon‚Äôt know‚Äù + zero degree (already solved)

Filipino subgroup analysis

‚úÖ The 980k aligns with your earlier EU/ONS bound, and you‚Äôve already solved the anomalies.
üîß Adjustment: You don‚Äôt need to rerun all sensitivity grids yet ‚Äî just fix on 980k as your working ‚Äúpreferred N.‚Äù Add sensitivity later for robustness.

3. Risk Index Implementation

Weights:

NRM referral (0.35)

Forced labor indicators (0.55 total)

Below minimum wage (0.10)




üö¶ Practical Recommendation

Given this, here‚Äôs the laser-focused order I‚Äôd suggest for you now:

Pick one outcome (say, ‚Äúpay below minimum wage‚Äù).

Run preferred RDS SS estimate (with N=980k, bootstrap CI).

Run weighted NSUM for same outcome (eq. 7 with SS weights, v=1,0.7,0.5).

Produce one table + forest plot in Quarto.

Insert stub commentary beneath.

Once this is running smoothly, extend to the other grouped questions, then to risk index, then to Filipino subgroup. This way you‚Äôll have something publishable in days, not months.




---




my best guess is you want me to lock in a concrete, step-by-step plan for producing the **Findings**‚Äîsettling on a preferred RDS specification, implementing the NSUM using your RDS data with proper weights, and then comparing both (with uncertainty). Here‚Äôs a tight blueprint you can execute now.

* * *

Step 1 ‚Äî Lock a ‚Äúpreferred‚Äù RDS specification (primary estimate)
================================================================

1. **Estimator & target**
    
    * Use **Successive Sampling (SS) RDS estimator** for the _proportion_ of DW experiencing MS in the past 12 months (your binary definitions: **Q36** under-minimum wage; **Q80** NRM referral). Treat each as a separate outcome, plus (optionally) a combined ‚ÄúMS if either=1‚Äù outcome.
        
    * Keep **VH/RDS-II** as a **secondary comparator** (appendix).
        
2. **Population size prior (N_DW)**
    
    * Center the SS prior on a **DW population bound from EU/ONS classifications (NACE 88 & 97)**; run sensitivity on a **low** bound (~5√ó10^4) and **high** bound (‚â•10^6). Earlier meetings/emails indicate this range and that results are insensitive once N is large, but you‚Äôll still show a sensitivity grid.
        
3. **Network degree & seeds**
    
    * Use cleaned **personal network size** (degree) with your completed treatments for ‚Äúdon‚Äôt knows‚Äù and zero-degree cases (already resolved).
        
    * Include seeds; report a **seed-exclusion sensitivity** in appendix.
        
4. **Homophily / clustering**
    
    * Preferred spec: pooled SS with post-strata (if justified) as a **robustness**; appendix models split by nationality/language (Filipina, Latinx, UK-born) only if recruitment trees suggest segmentation. Earlier notes flagged potential clustering; keep primary pooled unless diagnostics demand otherwise.
        
5. **Uncertainty**
    
    * Use an **RDS bootstrap** that respects recruitment (tree-aware), resampling recruiters and adjusting degrees; report **median** and **percentile 95% CI**. Keep MCMC/Bayesian SS variants (if any) as a robustness note, not the mainline.
        
6. **Diagnostics (report, not dwell)**
    
    * Convergence of estimates across waves, recruitment bottlenecks, homophily indices, seed dependence checks‚Äîconcise plots to supplement the main findings. Your prior convergence checks were positive.
        

**Deliverables from Step 1**

* Primary table: $\hat{p}_{\text{RDS-SS}}$ for each definition (Q36, Q80, ‚Äúeither‚Äù), with 95% CI; rows for sensitivity to $N_{DW}$.
    
* One figure: point + CI across $N_{DW}$ grid (e.g., 50k ‚Üí 1.5M).
    

* * *

Step 2 ‚Äî Implement NSUM using the RDS sample (weighted)
=======================================================

**Goal.** Estimate **the prevalence of MS within the DW population** using NSUM logic, where the _frame population_ = DW in the UK, and the _hidden subpopulation_ = DW experiencing MS.

1. **Quantities from your (already aggregated) survey items**
    
    * $d_i$: number of DW in the UK that respondent $i$ knows (ego DW-network size).
        
    * $y_i$: number of those alters the respondent believes **experienced MS** in the past year (you‚Äôve already _aggregated ego items to match alter definitions_‚Äîgood).
        
    * (Optional) visibility/awareness inputs‚Äîsee below.
        
2. **Inclusion probabilities & weights ($$\pi_i, w_i$$)**
    
    * Because the sample is RDS (not SRS), use **Horvitz‚ÄìThompson style weighting** per Feehan & Salganik (2016, eq. 7 idea): replace unweighted sums with **weighted sums** using $w_i \propto 1/\pi_i$.
        
    * Practical choice for $\pi_i$:
        
        * **Option A (simple):** $\pi_i \propto \text{degree}_i$ (VH/RDS-II logic).
            
        * **Option B (preferred):** **SS-based** $\pi_i$ from your chosen $N_{DW}$ prior (aligns RDS + NSUM assumptions).
            
    * Normalize $w_i$ to $\sum w_i = n$ (or 1); invariance holds.
        
3. **Core weighted NSUM estimator (within-DW prevalence)**
    
    $$\widehat{p}_{\text{NSUM}} \;=\; \frac{\sum_{i} w_i\, y_i / \widehat{v}}{\sum_{i} w_i\, d_i}$$
    * $\widehat{v}$ = **visibility/transmission factor** (probability ties are _aware_ of MS status).
        
        * Baseline: $\widehat{v}=1$ (upper bound on prevalence).
            
        * Sensitivity: grid over $\{0.5, 0.7, 1.0\}$.
            
        * If you have ‚Äúgame of contacts‚Äù / probe-alter visibility modules from survivors or knowledgeable alters, plug those to empirically estimate $\widehat{v}$. Your team discussed probe alters and the inapplicability of NRM referrals as alters; you settled on **‚Äú#DW known‚Äù** as the probe alter‚Äîthis slots in neatly here.
            
4. **Barrier/recall adjustments (optional, sensitivity)**
    
    * If you fielded items to assess barrier effects (differential mixing) or recall bias, include standard correction factors as a **secondary** sensitivity panel. Otherwise, transparently note as a limitation and bracket with visibility sensitivity.
        
5. **Uncertainty for NSUM**
    
    * Use a **design-consistent bootstrap**: resample recruitment trees (to respect RDS structure), regenerate weighted $\widehat{p}_{\text{NSUM}}$, and take percentile 95% CI.
        
    * Mirror the $N_{DW}$ prior grid and $\widehat{v}$ grid in the bootstrap to propagate uncertainty coherently.
        

**Deliverables from Step 2**

* Primary table: $\hat{p}_{\text{NSUM}}$ with 95% CI under preferred $N_{DW}$ and $\pi_i$ choice; columns for $\widehat{v}\in\{1.0,0.7,0.5\}$.
    
* Appendix: sensitivity to $\pi_i$ choice (VH vs SS), $N_{DW}$ grid.
    

* * *

Step 3 ‚Äî Side-by-side comparison (RDS vs NSUM)
==============================================

1. **Main comparison table**
    
    * Rows = outcome definitions (Q36, Q80, ‚Äúeither‚Äù).
        
    * Columns = $\hat{p}_{\text{RDS-SS}}$ [95% CI], $\hat{p}_{\text{NSUM}}$ [95% CI at $\widehat{v}=0.7$], and the **absolute difference** $|\Delta|=|\hat{p}_{\text{RDS}}-\hat{p}_{\text{NSUM}}|$.
        
2. **Visuals**
    
    * **Forest plot** of both estimates per definition.
        
    * **Sensitivity ribbons**: two slim panels showing how each method moves across $N_{DW}$ (x-axis) and across $\widehat{v}$ (for NSUM).
        
3. **Agreement diagnostics (brief)**
    
    * Report overlap of CIs and a **simple agreement metric** (e.g., mean absolute difference across definitions); no hypothesis-testing theatrics‚Äîkeep it descriptive.
        

**Deliverables from Step 3**

* One comparison figure + one table bound for the main text; full sensitivity grids in appendix.
    

* * *

Step 4 ‚Äî ‚ÄúFindings‚Äù section outline (ready to draft)
====================================================

1. **Sample & measures (1 short paragraph).**  
    RDS sample (Latinx, Filipina; UK DW), MS outcomes (Q36, Q80, either), risk index saved for Discussion framing. Data cleaning choices were finalized.
    
2. **Preferred RDS estimate.**  
    SS estimator, $N_{DW}$ prior choice & insensitivity once $N$ large, bootstrap CI; headline prevalence. Earlier work/meetings anticipated ~30‚Äì40% ranges; you‚Äôll now present the finalized numbers.
    
3. **NSUM on RDS with weights.**  
    Feehan‚ÄìSalganik weighted form (via $\pi_i$), probe-alter = ‚Äú#DW known‚Äù (not NRM), visibility sensitivity; headline prevalence.
    
4. **Comparison and robustness.**  
    Side-by-side point estimates + CIs; concise sensitivity statements (N prior, $\widehat{v}$, $\pi_i$ choice). Reserve plots/tables for appendix where needed.
    
5. **Key empirical takeaway (1‚Äì2 sentences).**  
    Are the methods broadly consistent? Where do they diverge (and under which assumptions)? One sentence on implications, with the fuller interpretation deferred to **Discussion**.
    
    
    
    ___
    
---
---











```{r setu, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
library(kableExtra)
library(ggplot2)
library(plotly)
library(DT)
library(gt)

# Load results
load("output/rds_estimates.RData")
load("data/processed/prepared_data.RData")
```

## Executive Summary

This analysis estimates the prevalence of modern slavery among domestic workers in the UK using multiple RDS methodologies. We examine two primary indicators across various population size assumptions and estimation techniques.

**Key Findings:**
- Below minimum wage prevalence (Q36): X.X% - Y.Y% (95% CI)
- NRM referral experience (Q80): A.A% - B.B% (95% CI)  
- Population size estimates: 980,000 - 1.74M domestic workers

## Sample Characteristics

### Recruitment Network Structure

```{r network-summary}
#| label: tbl-network
#| tbl-cap: "RDS Sample Network Characteristics"

network_stats <- tibble(
  Characteristic = c("Total Sample Size", "Recruitment Waves", "Average Degree", 
                    "Median Degree", "Seeds", "Longest Chain", "Mean Chain Length"),
  Value = c(
    nrow(rd.dd),
    max(rd.dd$wave, na.rm = TRUE),
    round(mean(rd.dd$numRef, na.rm = TRUE), 1),
    round(median(rd.dd$numRef, na.rm = TRUE), 1),
    sum(rd.dd$recruiter.id == -1),
    "TBD", # Calculate from recruitment chains
    "TBD"  # Calculate average chain length
  )
)

network_stats %>%
  gt() %>%
  tab_header(title = "Network Characteristics") %>%
  fmt_number(columns = Value, decimals = 1, use_seps = TRUE)
```

### Demographic Composition

```{r demographics}
#| label: fig-demographics
#| fig-cap: "Sample Composition by Nationality Clusters"

# Create nationality breakdown
nationality_summary <- rd.dd %>%
  group_by(nationality_cluster) %>%
  summarise(
    n = n(),
    percent = round(n() / nrow(rd.dd) * 100, 1),
    .groups = 'drop'
  )

ggplot(nationality_summary, aes(x = nationality_cluster, y = percent, fill = nationality_cluster)) +
  geom_col() +
  geom_text(aes(label = paste0(n, "\n(", percent, "%)")), vjust = -0.5) +
  scale_fill_viridis_d() +
  theme_minimal() +
  labs(x = "Nationality Cluster", y = "Percentage", 
       title = "Sample Distribution by Nationality") +
  theme(legend.position = "none")
```

## RDS Estimation Results

### Model-Assisted Estimates by Population Size

```{r ma-estimates-table}
#| label: tbl-ma-estimates
#| tbl-cap: "Model-Assisted Estimates by Population Size and Seed Selection"

# Extract MA results and create comparison table
ma_comparison <- expand_grid(
  seed_method = c("sample", "random", "degree"),
  pop_size = c(100000, 953000, 1000000, 1500000),
  indicator = c("q36", "q80", "composite_risk")
) %>%
  rowwise() %>%
  mutate(
    key = paste0(seed_method, "_", pop_size),
    estimate = ifelse(key %in% names(ma_results),
                     ma_results[[key]][[indicator]]$estimate, NA),
    ci_lower = ifelse(key %in% names(ma_results),
                     ma_results[[key]][[indicator]]$conf.int[1], NA),
    ci_upper = ifelse(key %in% names(ma_results),
                     ma_results[[key]][[indicator]]$conf.int[2], NA)
  ) %>%
  ungroup() %>%
  filter(!is.na(estimate))

# Create formatted table
ma_comparison %>%
  mutate(
    pop_size_f = scales::comma(pop_size),
    estimate_ci = sprintf("%.3f (%.3f, %.3f)", estimate, ci_lower, ci_upper)
  ) %>%
  select(seed_method, pop_size_f, indicator, estimate_ci) %>%
  pivot_wider(names_from = indicator, values_from = estimate_ci) %>%
  gt() %>%
  tab_header(title = "Model-Assisted Estimates by Method") %>%
  cols_label(
    seed_method = "Seed Selection",
    pop_size_f = "Population Size",
    q36 = "Below Min Wage",
    q80 = "NRM Experience", 
    composite_risk = "Composite Risk"
  ) %>%
  tab_spanner(label = "Prevalence Estimates (95% CI)", columns = c(q36, q80, composite_risk))
```

### Traditional RDS Estimators Comparison

```{r rds-comparison}
#| label: tbl-rds-comparison
#| tbl-cap: "RDS Estimator Comparison (N=980,000)"

# Extract RDS estimates for standard population size
rds_980k <- rds_results[["980000"]]

rds_table <- tibble(
  Method = c("RDS-I", "RDS-II", "Successive Sampling"),
  `Q36 Estimate` = c(
    rds_980k$rds_i$q36$estimate,
    rds_980k$rds_ii$q36$estimate,
    rds_980k$rds_ss$q36$estimate
  ),
  `Q36 95% CI` = c(
    sprintf("(%.3f, %.3f)", rds_980k$rds_i$q36$conf.int[1], rds_980k$rds_i$q36$conf.int[2]),
    sprintf("(%.3f, %.3f)", rds_980k$rds_ii$q36$conf.int[1], rds_980k$rds_ii$q36$conf.int[2]),
    sprintf("(%.3f, %.3f)", rds_980k$rds_ss$q36$conf.int[1], rds_980k$rds_ss$q36$conf.int[2])
  ) #...
```





Dear Georgia,
As I mentioned, there was no 'portal' in your previous emails.  I attach here proof of deposit, and some financials.  I am traveling and do not have the 'authenticator' to log into my checking account with Virgin Money.    I include staments from savings (Revoult).
The HR portal at work is not functioning right now and so I don't have access to the most recent pay slips.  I attach some dated ones that have acurate information.  _Please_ let me know what I need to do to get an AIP, and to close this deal.
As I mentioned, Nathan (CCed here) has stated there is a 10am Friday deadline at which time the vendor will make a decision.  I really really **REALLY** do not want to miss out on this purchaning opportunity, so I am hopeful we can move quickly on this.

Thank you,
scott



